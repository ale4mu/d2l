{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在二维互相关运算中，卷积窗口从输入张量的左上角开始，从左到右、从上到下滑动。\n",
    "当卷积窗口滑动到新一个位置时，包含在该窗口中的部分张量与卷积核张量进行按元素相乘，得到的张量再求和得到一个单一的标量值，由此我们得出了这一位置的输出张量值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19., 25.],\n",
      "        [37., 43.]])\n",
      "tensor([[19.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 互相关运算\n",
    "def corr2d(X,K,stride = 1):\n",
    "    height,width = K.shape # 卷积核的高度和宽度\n",
    "    Y = torch.zeros(((X.shape[0]-height) // stride+1,(X.shape[1]-width) // stride+1)) # 输出的大小\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i,j] = (X[i*stride:i*stride+height,j*stride:j*stride+width] * K).sum()\n",
    "    return Y\n",
    "\n",
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "print(corr2d(X, K))\n",
    "print(corr2d(X, K, stride = 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积层\n",
    "\n",
    "卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。\n",
    "所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。\n",
    "就像我们之前随机初始化全连接层一样，在训练基于卷积层的模型时，我们也随机初始化卷积核权重。\n",
    "\n",
    "基于上面定义的`corr2d`函数[**实现二维卷积层**]。在`__init__`构造函数中，将`weight`和`bias`声明为两个模型参数。前向传播函数调用`corr2d`函数并添加偏置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在许多情况下，我们需要设置$p_h=k_h-1$和$p_w=k_w-1$，使输入和输出具有相同的高度和宽度。\n",
    "此外，使用奇数的核大小和填充大小也提供了书写上的便利。对于任何二维张量`X`，当满足：\n",
    "1. 卷积核的大小是奇数；\n",
    "2. 所有边的填充行数和列数相同；\n",
    "3. 输出与输入具有相同高度和宽度\n",
    "则可以得出：输出`Y[i, j]`是通过以输入`X[i, j]`为中心，与卷积核进行互相关计算得到的。\n",
    "\n",
    "- 通常，当垂直步幅为$s_h$、水平步幅为$s_w$时，输出形状为\n",
    "\n",
    "$$\\lfloor(n_h-k_h+p_h+s_h)/s_h\\rfloor \\times \\lfloor(n_w-k_w+p_w+s_w)/s_w\\rfloor.$$\n",
    "\n",
    "如果我们设置了$p_h=k_h-1$和$p_w=k_w-1$，则输出形状将简化为$\\lfloor(n_h+s_h-1)/s_h\\rfloor \\times \\lfloor(n_w+s_w-1)/s_w\\rfloor$。\n",
    "更进一步，如果输入的高度和宽度可以被垂直和水平步幅整除，则输出形状将为$(n_h/s_h) \\times (n_w/s_w)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class MyConv2D(nn.Module):\n",
    "    def __init__(self, kernel_size,padding = 0,stride = 1):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self,x):\n",
    "        if self.padding > 0:\n",
    "            x = torch.nn.functional.pad(x,(self.padding,self.padding,self.padding,self.padding))\n",
    "        return corr2d(x,self.weight,stride = self.stride) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.1771, 0.6950, 0.6815],\n",
       "        [0.5314, 2.4639, 4.0908, 2.4553],\n",
       "        [3.2623, 7.3444, 8.9713, 4.6050],\n",
       "        [4.3990, 7.3869, 8.4959, 3.0064]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyConv2D(kernel_size=(2,2),padding = 1)\n",
    "model(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入和卷积核都有$c_i$个通道，我们可以对每个通道输入的二维张量和卷积核的二维张量进行互相关运算，再对通道求和（将$c_i$的结果相加）得到二维张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  72.],\n",
       "        [104., 120.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 多通道输入\n",
    "def corr2d_multi_in(X, K):\n",
    "    # 先遍历“X”和“K”的第0个维度（通道维度），再把它们加在一起\n",
    "    return sum(corr2d(x, k) for x, k in zip(X, K))\n",
    "\n",
    "X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
    "               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
    "K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
    "corr2d_multi_in(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用$c_i$和$c_o$分别表示输入和输出通道的数目，并让$k_h$和$k_w$为卷积核的高度和宽度。为了获得多个通道的输出，我们可以为每个输出通道创建一个形状为$c_i\\times k_h\\times k_w$的卷积核张量，这样卷积核的形状是$c_o\\times c_i\\times k_h\\times k_w$。在互相关运算中，每个输出通道先获取所有输入通道，再以对应该输出通道的卷积核计算出结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 56.,  72.],\n",
       "         [104., 120.]],\n",
       "\n",
       "        [[ 76., 100.],\n",
       "         [148., 172.]],\n",
       "\n",
       "        [[ 96., 128.],\n",
       "         [192., 224.]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 多通道输出\n",
    "def corr2d_multi_in_out(X, K):\n",
    "    # 迭代“K”的第0个维度，每次都对输入“X”执行互相关运算。\n",
    "    # 最后将所有结果都叠加在一起\n",
    "    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)\n",
    "K = torch.stack((K, K + 1, K + 2), 0) #三通道输出\n",
    "corr2d_multi_in_out(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 与互相关运算符一样，汇聚窗口从输入张量的左上角开始，从左往右、从上往下的在输入张量内滑动。在汇聚窗口到达的每个位置，它计算该窗口中输入子张量的最大值或平均值\n",
    "- 默认情况下，(**深度学习框架中的步幅与汇聚窗口的大小相同**)。\n",
    "- 在处理多通道输入数据时，[**汇聚层在每个输入通道上单独运算**]，而不是像卷积层一样在通道上对输入进行汇总。\n",
    "这意味着汇聚层的输出通道数与输入通道数相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24., 30.],\n",
      "        [42., 48.]])\n",
      "tensor([[ 7.7500, 10.7500],\n",
      "        [16.7500, 19.7500]])\n"
     ]
    }
   ],
   "source": [
    "def pool2d(X,pool_size,stride = 1,mode = 'max'):\n",
    "    height,width = pool_size\n",
    "    Y = torch.zeros(((X.shape[0]-height) // stride+1,(X.shape[1]-width) // stride+1)) \n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i,j] = (X[i*stride:i*stride+height,j*stride:j*stride+width] * K).max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i,j] = (X[i*stride:i*stride+height,j*stride:j*stride+width] * K).mean()\n",
    "\n",
    "    return Y\n",
    "\n",
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "print(pool2d(X, (2, 2)))\n",
    "print(pool2d(X, (2, 2),stride = 1,mode = 'avg'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
